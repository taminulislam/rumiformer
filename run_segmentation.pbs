#!/bin/bash
#PBS -l select=1:system=polaris
#PBS -l walltime=24:00:00
#PBS -q capacity
#PBS -A CXR-Images-Radiology-Reports-Lung-Diseases-Classification
#PBS -N rumiformer_seg
#PBS -l filesystems=home:eagle
#PBS -r y
#PBS -o /home/taminul_islam/Co2_farm/logs/segmentation_stdout.log
#PBS -e /home/taminul_islam/Co2_farm/logs/segmentation_stderr.log

set -e

export OPENBLAS_NUM_THREADS=8
export OMP_NUM_THREADS=8

export http_proxy="http://proxy.alcf.anl.gov:3128"
export https_proxy="http://proxy.alcf.anl.gov:3128"
export ftp_proxy="http://proxy.alcf.anl.gov:3128"
export HF_HUB_DISABLE_XET=1

eval "$(~/miniconda3/bin/conda shell.bash hook 2>/dev/null)"
conda activate rumiformer

cd /home/taminul_islam/Co2_farm

echo "========================================="
echo "Job ID: $PBS_JOBID"
echo "Node: $(hostname)"
echo "Date: $(date)"
echo "========================================="
python -c "
import torch
print(f'PyTorch: {torch.__version__}')
print(f'CUDA available: {torch.cuda.is_available()}')
print(f'GPUs: {torch.cuda.device_count()}')
for i in range(torch.cuda.device_count()):
    print(f'  GPU {i}: {torch.cuda.get_device_name(i)} ({torch.cuda.get_device_properties(i).total_memory / 1e9:.1f} GB)')
"
echo "========================================="

# 4-GPU DDP training
torchrun --nproc_per_node=4 --standalone \
    src/train/train_segmentation.py --resume

echo "========================================="
echo "Stage 1 (Segmentation) finished at $(date)"
echo "========================================="
